"""
Smart Insights Page for SeeSense Dashboard - DYNAMIC Version
AI-powered safety analysis that responds to date filters from Overview page
All insights are computed dynamically based on filtered data
"""
import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from sklearn.ensemble import RandomForestRegressor, IsolationForest
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, r2_score
from datetime import datetime, timedelta
from typing import Dict, Any, Optional, Tuple, List
import logging
import warnings

from app.core.data_processor import data_processor
from app.utils.config import config

# Suppress technical warnings for cleaner output
warnings.filterwarnings('ignore', category=UserWarning)
logger = logging.getLogger(__name__)


def render_smart_insights_page():
    """Render the Smart Insights page with DYNAMIC analysis based on date filters"""
    st.title("ğŸ§  Smart Insights")
    st.markdown("**AI discovers actionable patterns in your cycling data to keep you safer**")
    
    # Show active date filter information at the top
    show_active_filters()
    
    # Add helpful explanation with modern styling
    with st.expander("â„¹ï¸ What are Smart Insights?", expanded=False):
        st.markdown("""
        <div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 20px; border-radius: 10px; color: white; margin: 10px 0;'>
        <h4 style='color: white; margin-top: 0;'>ğŸ¤– Your Personal Safety AI</h4>
        Our advanced AI analyzes your cycling data to discover hidden patterns and predict safety risks.
        Think of it as having a smart cycling coach that learns from thousands of rides!
        </div>
        
        **What you'll discover:**
        - ğŸ¯ **Safety Predictions** - Which conditions lead to higher risks
        - ğŸ‘¥ **Riding Patterns** - Your unique cycling personality and habits  
        - âš ï¸ **Safety Alerts** - When conditions become unusually risky
        - ğŸ“Š **Smart Factors** - What really affects your safety (and what doesn't)
        
        **ğŸ“… Dynamic Analysis:** Results automatically update based on your date selection in the Overview page!
        """, unsafe_allow_html=True)
    
    try:
        # Load and filter all datasets dynamically
        routes_df, braking_df, swerving_df, time_series_df = load_and_filter_data()
        
        # Check if we have any data after filtering
        if not has_sufficient_data(routes_df, braking_df, swerving_df, time_series_df):
            render_no_data_message()
            return
        
        # Add dynamic controls in sidebar
        smart_options = render_dynamic_controls(time_series_df)
        
        # Create modern tabs with emojis
        safety_tab, patterns_tab, alerts_tab, insights_tab = st.tabs([
            "ğŸ¯ Safety Intelligence", 
            "ğŸ‘¥ Your Cycling DNA", 
            "âš ï¸ Smart Alerts", 
            "ğŸ§¬ Safety Factors"
        ])
        
        with safety_tab:
            render_dynamic_safety_intelligence(routes_df, braking_df, swerving_df, time_series_df, smart_options)
        
        with patterns_tab:
            render_dynamic_cycling_dna(routes_df, time_series_df, smart_options)
        
        with alerts_tab:
            render_dynamic_smart_alerts(time_series_df, braking_df, swerving_df, smart_options)
        
        with insights_tab:
            render_dynamic_safety_factors_analysis(routes_df, braking_df, swerving_df, time_series_df, smart_options)
        
    except Exception as e:
        logger.error(f"Error in Smart Insights page: {e}")
        st.error("âš ï¸ Something went wrong while analyzing your data.")
        st.info("Please check your data files and try refreshing the page.")
        
        with st.expander("ğŸ” Technical Details"):
            st.code(str(e))


def show_active_filters():
    """Show information about active date filters"""
    start_date = st.session_state.get('filter_start_date') or st.session_state.get('overview_date_filter')
    end_date = st.session_state.get('filter_end_date')
    
    # Try to get date range from overview filter
    if not start_date and 'overview_date_filter' in st.session_state:
        date_range = st.session_state.get('overview_date_filter')
        if isinstance(date_range, (list, tuple)) and len(date_range) == 2:
            start_date, end_date = date_range
    
    if start_date and end_date:
        st.info(f"ğŸ“… **Dynamic Analysis Period:** {start_date} to {end_date}")
    else:
        st.info("ğŸ“… **Analysis:** Full dataset (no date filters applied)")


def load_and_filter_data():
    """Load all datasets and apply date filters dynamically"""
    # Load all datasets
    all_data = data_processor.load_all_datasets()
    
    # Extract dataframes
    routes_df = all_data.get('routes', (None, {}))[0]
    braking_df = all_data.get('braking_hotspots', (None, {}))[0]
    swerving_df = all_data.get('swerving_hotspots', (None, {}))[0]
    time_series_df = all_data.get('time_series', (None, {}))[0]
    
    # Apply date filters from session state (set by Overview page or stored filters)
    routes_df, braking_df, swerving_df, time_series_df = apply_dynamic_date_filters(
        routes_df, braking_df, swerving_df, time_series_df
    )
    
    return routes_df, braking_df, swerving_df, time_series_df


def apply_dynamic_date_filters(routes_df, braking_df, swerving_df, time_series_df):
    """Apply date filters from session state to all dataframes with multiple fallback options"""
    try:
        # Try multiple sources for date filters
        start_date = None
        end_date = None
        
        # Option 1: Direct filter dates from overview
        if 'filter_start_date' in st.session_state and 'filter_end_date' in st.session_state:
            start_date = st.session_state['filter_start_date'] 
            end_date = st.session_state['filter_end_date']
        
        # Option 2: Date range from overview filter widget
        elif 'overview_date_filter' in st.session_state:
            date_range = st.session_state['overview_date_filter']
            if isinstance(date_range, (list, tuple)) and len(date_range) == 2:
                start_date, end_date = date_range
        
        # Option 3: Look for any date-related keys in session state
        else:
            for key, value in st.session_state.items():
                if 'date' in key.lower() and isinstance(value, (list, tuple)) and len(value) == 2:
                    start_date, end_date = value
                    break
        
        if start_date is None or end_date is None:
            return routes_df, braking_df, swerving_df, time_series_df
        
        # Convert to datetime for comparison
        start_date = pd.to_datetime(start_date)
        end_date = pd.to_datetime(end_date)
        
        # Store in session state for consistency
        st.session_state['filter_start_date'] = start_date
        st.session_state['filter_end_date'] = end_date
        
        # Filter time series data
        if time_series_df is not None and 'date' in time_series_df.columns:
            time_series_df = time_series_df.copy()
            time_series_df['date'] = pd.to_datetime(time_series_df['date'])
            mask = (time_series_df['date'] >= start_date) & (time_series_df['date'] <= end_date)
            time_series_df = time_series_df[mask]
            logger.info(f"Filtered time series from {len(time_series_df)} to {mask.sum()} records")
        
        # Filter braking hotspots data
        if braking_df is not None and 'date_recorded' in braking_df.columns:
            braking_df = braking_df.copy()
            braking_df['date_recorded'] = pd.to_datetime(braking_df['date_recorded'])
            mask = (braking_df['date_recorded'] >= start_date) & (braking_df['date_recorded'] <= end_date)
            braking_df = braking_df[mask]
        
        # Filter swerving hotspots data
        if swerving_df is not None and 'date_recorded' in swerving_df.columns:
            swerving_df = swerving_df.copy()
            swerving_df['date_recorded'] = pd.to_datetime(swerving_df['date_recorded'])
            mask = (swerving_df['date_recorded'] >= start_date) & (swerving_df['date_recorded'] <= end_date)
            swerving_df = swerving_df[mask]
        
        # Note: Routes data doesn't typically have dates, so we keep it as is
        # unless there's a specific date column
        if routes_df is not None and 'date' in routes_df.columns:
            routes_df = routes_df.copy()
            routes_df['date'] = pd.to_datetime(routes_df['date'])
            mask = (routes_df['date'] >= start_date) & (routes_df['date'] <= end_date)
            routes_df = routes_df[mask]
        
        return routes_df, braking_df, swerving_df, time_series_df
        
    except Exception as e:
        logger.error(f"Error applying date filters: {e}")
        return routes_df, braking_df, swerving_df, time_series_df


def has_sufficient_data(routes_df, braking_df, swerving_df, time_series_df):
    """Check if we have sufficient data after filtering"""
    # Check if at least one dataset has meaningful data
    datasets_with_data = 0
    
    if routes_df is not None and len(routes_df) > 0:
        datasets_with_data += 1
    if braking_df is not None and len(braking_df) > 0:
        datasets_with_data += 1
    if swerving_df is not None and len(swerving_df) > 0:
        datasets_with_data += 1
    if time_series_df is not None and len(time_series_df) > 5:  # Need at least 5 days
        datasets_with_data += 1
    
    return datasets_with_data > 0


def render_no_data_message():
    """Render modern no-data message for filtered period"""
    st.markdown("""
    <div style='text-align: center; padding: 40px; background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); border-radius: 20px; color: white;'>
    <h2 style='color: white;'>ğŸ“… No Data for Selected Period</h2>
    <p style='font-size: 18px; margin: 20px 0;'>Try expanding your date range or check if data exists for this time period!</p>
    </div>
    """, unsafe_allow_html=True)
    
    st.markdown("""
    ## ğŸ” What to Try
    
    **ğŸ“ˆ Expand Date Range** - Go back to Overview and select a broader time period  
    **ğŸ“Š Check Data Coverage** - Ensure your CSV files contain data for the selected dates  
    **ğŸ”„ Remove Filters** - Try viewing insights without date filters first
    
    Once you adjust the date range, the AI insights will automatically update! ğŸ‰
    """)


def render_dynamic_controls(time_series_df):
    """Render dynamic controls that adapt to the filtered data"""
    st.sidebar.markdown("""
    <div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 15px; border-radius: 10px; color: white; margin-bottom: 20px;'>
    <h3 style='color: white; margin: 0;'>âš™ï¸ AI Settings</h3>
    <p style='margin: 5px 0 0 0; font-size: 12px;'>Adapted to your filtered data</p>
    </div>
    """, unsafe_allow_html=True)
    
    options = {}
    
    # Calculate data-driven defaults
    data_size = len(time_series_df) if time_series_df is not None else 10
    
    # Sensitivity based on data size
    default_sensitivity = 1 if data_size < 10 else 1 if data_size < 30 else 1  # Balanced for most cases
    
    options['sensitivity'] = st.sidebar.radio(
        "ğŸ” Alert Sensitivity",
        ["ğŸŸ¢ Relaxed (10%)", "ğŸŸ¡ Balanced (5%)", "ğŸ”´ Vigilant (2%)"],
        index=default_sensitivity,
        help=f"Based on {data_size} data points in your filtered period"
    )
    
    # Convert to technical values
    sensitivity_map = {"ğŸŸ¢ Relaxed (10%)": 0.1, "ğŸŸ¡ Balanced (5%)": 0.05, "ğŸ”´ Vigilant (2%)": 0.02}
    options['anomaly_contamination'] = sensitivity_map[options['sensitivity']]
    
    # Prediction period based on data timespan
    if time_series_df is not None and len(time_series_df) > 0 and 'date' in time_series_df.columns:
        try:
            time_series_df['date'] = pd.to_datetime(time_series_df['date'])
            timespan_days = (time_series_df['date'].max() - time_series_df['date'].min()).days
            default_prediction = 2 if timespan_days < 30 else 2  # Default to month
        except:
            default_prediction = 2
    else:
        default_prediction = 2
    
    options['prediction_period'] = st.sidebar.selectbox(
        "ğŸ”® Prediction Horizon",
        ["ğŸ“… Next Week", "ğŸ“Š Next 2 Weeks", "ğŸ“ˆ Next Month", "ğŸ¯ Next Quarter"],
        index=default_prediction,
        help="AI predictions based on patterns in your filtered data"
    )
    
    # Convert to days
    period_map = {"ğŸ“… Next Week": 7, "ğŸ“Š Next 2 Weeks": 14, "ğŸ“ˆ Next Month": 30, "ğŸ¯ Next Quarter": 90}
    options['prediction_days'] = period_map[options['prediction_period']]
    
    # Pattern detail based on data richness
    if data_size < 15:
        default_detail = 0  # Simple
    elif data_size < 50:
        default_detail = 1  # Moderate
    else:
        default_detail = 2  # Detailed
    
    options['pattern_detail'] = st.sidebar.selectbox(
        "ğŸ¨ Pattern Detail",
        ["ğŸ” Simple (2-3 patterns)", "âš–ï¸ Moderate (4-5 patterns)", "ğŸ¯ Detailed (6-8 patterns)"],
        index=default_detail,
        help=f"Optimized for {data_size} data points"
    )
    
    # Convert to clusters
    detail_map = {"ğŸ” Simple (2-3 patterns)": 3, "âš–ï¸ Moderate (4-5 patterns)": 4, "ğŸ¯ Detailed (6-8 patterns)": 6}
    options['n_clusters'] = detail_map[options['pattern_detail']]
    
    # Dynamic minimum data requirement
    options['min_data_needed'] = max(5, min(30, data_size // 3))
    
    # Show data info
    st.sidebar.markdown(f"**ğŸ“Š Filtered Data:** {data_size} records")
    
    return options


def get_meaningful_features(df):
    """Extract only meaningful features for analysis, excluding coordinates and IDs"""
    if df is None or df.empty:
        return []
    
    # Define meaningful feature categories
    meaningful_patterns = [
        'speed', 'duration', 'distance', 'incidents', 'braking', 'swerving', 
        'temperature', 'precipitation', 'wind', 'visibility', 'intensity',
        'popularity', 'rating', 'days_active', 'cyclists', 'severity'
    ]
    
    # Filter columns to only meaningful ones
    all_columns = df.columns.tolist()
    meaningful_columns = []
    
    for col in all_columns:
        col_lower = col.lower()
        # Include if matches meaningful patterns and exclude coordinates/IDs
        if any(pattern in col_lower for pattern in meaningful_patterns):
            if not any(exclude in col_lower for exclude in ['lat', 'lon', 'id', '_id', 'start_', 'end_']):
                if df[col].dtype in ['int64', 'float64', 'int32', 'float32']:
                    meaningful_columns.append(col)
    
    return meaningful_columns


def render_dynamic_safety_intelligence(routes_df, braking_df, swerving_df, time_series_df, options):
    """Render safety predictions that adapt to filtered data"""
    st.markdown("### ğŸ¯ Safety Intelligence")
    
    # Create dynamic AI insight card
    data_period_info = get_data_period_info(time_series_df)
    
    with st.container():
        st.markdown(f"""
        <div style='background: linear-gradient(135deg, #ff9a9e 0%, #fecfef 100%); padding: 20px; border-radius: 15px; margin: 20px 0;'>
        <h4 style='margin-top: 0; color: #333;'>ğŸ¤– AI Insight</h4>
        <p style='font-size: 16px; margin-bottom: 0; color: #555;'>Analyzing {data_period_info} to predict when and where you're most at risk...</p>
        </div>
        """, unsafe_allow_html=True)
    
    # Use time series data as primary source for meaningful analysis
    primary_df = time_series_df if time_series_df is not None and len(time_series_df) > options['min_data_needed'] else routes_df
    
    if primary_df is None or len(primary_df) < options['min_data_needed']:
        st.info(f"ğŸ”„ Need at least {options['min_data_needed']} records for reliable predictions. Current period has {len(primary_df) if primary_df is not None else 0} records.")
        return
    
    # Get meaningful features only
    meaningful_features = get_meaningful_features(primary_df)
    
    if len(meaningful_features) < 2:
        st.warning("ğŸ” Not enough meaningful cycling metrics in the selected period for safety predictions!")
        return
    
    # Create safety predictions with meaningful variables
    prediction_results = create_dynamic_safety_predictions(primary_df, meaningful_features, options)
    
    if prediction_results is None:
        st.warning(f"ğŸ¤” Our AI couldn't find clear patterns in the selected {data_period_info.lower()}. Try a different time period!")
        return
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("#### ğŸ† What Drives Your Safety")
        
        importance_data = prediction_results['feature_importance']
        
        # Create beautiful, meaningful chart
        fig = px.bar(
            importance_data.head(8),  # Top 8 factors
            x='importance',
            y='friendly_name',
            orientation='h',
            title=f"Safety Factors ({data_period_info})",
            labels={'importance': 'Impact Level', 'friendly_name': ''},
            color='importance',
            color_continuous_scale='Viridis',
            text='importance'
        )
        fig.update_traces(texttemplate='%{text:.1%}', textposition='outside')
        fig.update_layout(height=400, showlegend=False, font=dict(size=12))
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        st.markdown("#### ğŸ² Your Safety Score Range")
        
        predictions = prediction_results['predictions']
        safety_scores = np.clip(10 - (predictions * 10), 1, 10)  # Convert to 1-10 scale
        
        # Create modern histogram
        fig = px.histogram(
            x=safety_scores,
            nbins=15,
            title=f"Safety Score Distribution ({data_period_info})",
            labels={'x': 'Safety Score (1=High Risk, 10=Very Safe)', 'y': 'Frequency'},
            color_discrete_sequence=['#6366f1']
        )
        fig.update_layout(height=400)
        st.plotly_chart(fig, use_container_width=True)
        
        # Add dynamic metrics
        avg_score = np.mean(safety_scores)
        score_std = np.std(safety_scores)
        
        col2a, col2b = st.columns(2)
        with col2a:
            st.metric(
                "ğŸ… Average Safety Score", 
                f"{avg_score:.1f}/10",
                help=f"Your safety level for {data_period_info.lower()}"
            )
        with col2b:
            consistency = "High" if score_std < 1 else "Medium" if score_std < 2 else "Variable"
            st.metric(
                "ğŸ“Š Consistency",
                consistency,
                help=f"Safety consistency during {data_period_info.lower()}"
            )
    
    # AI-generated insight with period context
    generate_dynamic_safety_intelligence_insight(prediction_results, safety_scores, data_period_info)


def render_dynamic_cycling_dna(routes_df, time_series_df, options):
    """Render cycling personality analysis adapted to filtered data"""
    st.markdown("### ğŸ‘¥ Your Cycling DNA")
    
    # Dynamic AI insight card
    data_period_info = get_data_period_info(time_series_df)
    
    with st.container():
        st.markdown(f"""
        <div style='background: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%); padding: 20px; border-radius: 15px; margin: 20px 0;'>
        <h4 style='margin-top: 0; color: #333;'>ğŸ§¬ AI Insight</h4>
        <p style='font-size: 16px; margin-bottom: 0; color: #555;'>Discovering your cycling personality patterns during {data_period_info.lower()}...</p>
        </div>
        """, unsafe_allow_html=True)
    
    # Use time series for richer pattern analysis
    primary_df = time_series_df if time_series_df is not None and len(time_series_df) > options['min_data_needed'] else routes_df
    
    if primary_df is None or len(primary_df) < options['min_data_needed']:
        st.info(f"ğŸ§¬ Building your cycling DNA profile... Need more data from {data_period_info.lower()}!")
        return
    
    # Get meaningful features
    meaningful_features = get_meaningful_features(primary_df)
    
    if len(meaningful_features) < 2:
        st.warning(f"ğŸ” Not enough cycling metrics in {data_period_info.lower()} to determine patterns!")
        return
    
    # Analyze cycling patterns
    pattern_results = analyze_dynamic_cycling_dna(primary_df, meaningful_features, options['n_clusters'], data_period_info)
    
    if pattern_results is None:
        st.warning(f"ğŸ¤” Your cycling patterns during {data_period_info.lower()} are still emerging. Keep riding!")
        return
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("#### ğŸ­ Your Cycling Personas")
        
        if 'persona_distribution' in pattern_results:
            persona_data = pattern_results['persona_distribution']
            
            # Create stunning pie chart
            fig = px.pie(
                persona_data,
                values='percentage',
                names='persona',
                title=f"Time Allocation ({data_period_info})",
                color_discrete_sequence=px.colors.qualitative.Set3
            )
            fig.update_traces(textposition='inside', textinfo='percent+label')
            fig.update_layout(height=400, font=dict(size=12))
            st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        st.markdown("#### ğŸ“ˆ Pattern Evolution")
        
        if 'pattern_timeline' in pattern_results and time_series_df is not None and len(time_series_df) > 7:
            timeline_data = pattern_results['pattern_timeline']
            
            fig = px.line(
                timeline_data,
                x='date',
                y='dominant_persona',
                title=f"Style Evolution ({data_period_info})",
                labels={'dominant_persona': 'Primary Cycling Style', 'date': 'Date'},
                color_discrete_sequence=['#8b5cf6']
            )
            fig.update_layout(height=400)
            st.plotly_chart(fig, use_container_width=True)
        else:
            # Show personality traits instead
            st.markdown(f"**ğŸ¯ Your Traits ({data_period_info}):**")
            if 'personality_traits' in pattern_results:
                for trait in pattern_results['personality_traits']:
                    st.markdown(f"âœ¨ {trait}")
    
    # AI-generated insight with period context
    generate_dynamic_cycling_dna_insight(pattern_results, data_period_info)


def render_dynamic_smart_alerts(time_series_df, braking_df, swerving_df, options):
    """Render intelligent safety alerts adapted to filtered period"""
    st.markdown("### âš ï¸ Smart Safety Alerts")
    
    # Dynamic AI insight card
    data_period_info = get_data_period_info(time_series_df)
    
    with st.container():
        st.markdown(f"""
        <div style='background: linear-gradient(135deg, #ffecd2 0%, #fcb69f 100%); padding: 20px; border-radius: 15px; margin: 20px 0;'>
        <h4 style='margin-top: 0; color: #333;'>ğŸ”® AI Insight</h4>
        <p style='font-size: 16px; margin-bottom: 0; color: #555;'>Monitoring unusual patterns during {data_period_info.lower()}...</p>
        </div>
        """, unsafe_allow_html=True)
    
    if time_series_df is None or len(time_series_df) < options['min_data_needed']:
        st.info(f"â³ Setting up smart monitoring... Need more data from {data_period_info.lower()} to detect patterns!")
        return
    
    # Get meaningful features for anomaly detection
    meaningful_features = get_meaningful_features(time_series_df)
    
    if len(meaningful_features) < 2:
        st.warning(f"ğŸ” Need more safety metrics from {data_period_info.lower()} to detect unusual patterns!")
        return
    
    # Detect smart alerts
    alert_results = detect_dynamic_intelligent_alerts(time_series_df, meaningful_features, options, data_period_info)
    
    if alert_results is None:
        st.warning(f"ğŸ¤” No unusual patterns detected during {data_period_info.lower()}!")
        return
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("#### ğŸš¨ Period Safety Alerts")
        
        if 'priority_alerts' in alert_results and len(alert_results['priority_alerts']) > 0:
            alerts = alert_results['priority_alerts']
            
            for alert in alerts[:5]:  # Show top 5 alerts
                severity_color = "ğŸ”´" if alert['severity'] > 0.7 else "ğŸŸ¡" if alert['severity'] > 0.4 else "ğŸŸ¢"
                
                st.markdown(f"""
                <div style='border-left: 4px solid {"#ef4444" if alert["severity"] > 0.7 else "#f59e0b" if alert["severity"] > 0.4 else "#10b981"}; 
                           padding: 15px; margin: 10px 0; background: #f8fafc; border-radius: 0 8px 8px 0;'>
                <strong>{severity_color} {alert['title']}</strong><br>
                <span style='color: #64748b;'>{alert['description']}</span><br>
                <small style='color: #94a3b8;'>ğŸ“… {alert['date']}</small>
                </div>
                """, unsafe_allow_html=True)
        else:
            st.markdown(f"""
            <div style='text-align: center; padding: 30px; background: linear-gradient(135deg, #d4edda 0%, #c3e6cb 100%); border-radius: 15px;'>
            <h4 style='color: #155724; margin-top: 0;'>ğŸ‰ All Clear!</h4>
            <p style='color: #155724; margin-bottom: 0;'>No safety alerts detected during {data_period_info.lower()}. Your rides have been consistently safe!</p>
            </div>
            """, unsafe_allow_html=True)
    
    with col2:
        st.markdown("#### ğŸ“Š Alert Trends")
        
        if 'alert_timeline' in alert_results:
            timeline = alert_results['alert_timeline']
            
            fig = px.area(
                timeline,
                x='date',
                y='alert_count',
                title=f"Alert Trends ({data_period_info})",
                labels={'alert_count': 'Daily Alerts', 'date': 'Date'},
                color_discrete_sequence=['#f59e0b']
            )
            fig.update_layout(height=400)
            st.plotly_chart(fig, use_container_width=True)
        
        # Dynamic summary metrics
        if 'summary_stats' in alert_results:
            stats = alert_results['summary_stats']
            
            st.metric(
                f"ğŸš¨ Alerts ({data_period_info})", 
                stats.get('period_alerts', 0),
                help=f"Safety alerts during {data_period_info.lower()}"
            )
            
            safe_percentage = stats.get('safe_percentage', 100)
            st.metric(
                "âœ… Safe Days", 
                f"{safe_percentage:.0f}%",
                help=f"Percentage of safe days in {data_period_info.lower()}"
            )
    
    # AI-generated insight with period context
    generate_dynamic_smart_alerts_insight(alert_results, data_period_info)


def render_dynamic_safety_factors_analysis(routes_df, braking_df, swerving_df, time_series_df, options):
    """Render intelligent analysis of safety factors for filtered period"""
    st.markdown("### ğŸ§¬ What Really Affects Your Safety")
    
    # Dynamic AI insight card
    data_period_info = get_data_period_info(time_series_df)
    
    with st.container():
        st.markdown(f"""
        <div style='background: linear-gradient(135deg, #e0c3fc 0%, #9bb5ff 100%); padding: 20px; border-radius: 15px; margin: 20px 0;'>
        <h4 style='margin-top: 0; color: #333;'>âš—ï¸ AI Insight</h4>
        <p style='font-size: 16px; margin-bottom: 0; color: #555;'>Uncovering safety connections during {data_period_info.lower()}...</p>
        </div>
        """, unsafe_allow_html=True)
    
    # Use time series for comprehensive factor analysis
    primary_df = time_series_df if time_series_df is not None and len(time_series_df) > options['min_data_needed'] else routes_df
    
    if primary_df is None or len(primary_df) < options['min_data_needed']:
        st.info(f"ğŸ”¬ Preparing factor analysis... Need more data from {data_period_info.lower()}!")
        return
    
    # Get meaningful features
    meaningful_features = get_meaningful_features(primary_df)
    
    if len(meaningful_features) < 3:
        st.warning(f"ğŸ” Need more safety metrics from {data_period_info.lower()} to analyze relationships!")
        return
    
    # Analyze safety factors with meaningful variables only
    factor_results = analyze_dynamic_intelligent_safety_factors(primary_df, meaningful_features, data_period_info)
    
    if factor_results is None:
        st.warning(f"ğŸ¤” Couldn't find clear relationships between safety factors during {data_period_info.lower()}!")
        return
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("#### ğŸ¯ Safety Factor Rankings")
        
        if 'factor_rankings' in factor_results:
            rankings = factor_results['factor_rankings']
            
            # Create modern ranking chart
            fig = px.bar(
                rankings.head(10),
                x='impact_score',
                y='factor_name',
                orientation='h',
                title=f"Factor Impact ({data_period_info})",
                labels={'impact_score': 'Safety Impact Score', 'factor_name': ''},
                color='impact_score',
                color_continuous_scale='Plasma',
                text='impact_score'
            )
            fig.update_traces(texttemplate='%{text:.2f}', textposition='outside')
            fig.update_layout(height=500, showlegend=False)
            st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        st.markdown("#### ğŸ”— Factor Connections")
        
        if 'smart_correlations' in factor_results:
            correlations = factor_results['smart_correlations']
            
            # Create network-style correlation chart
            fig = px.scatter(
                correlations,
                x='factor_1_impact',
                y='factor_2_impact', 
                size='connection_strength',
                color='relationship_type',
                title=f"Factor Relationships ({data_period_info})",
                labels={
                    'factor_1_impact': 'Factor 1 Impact',
                    'factor_2_impact': 'Factor 2 Impact',
                    'connection_strength': 'Connection Strength'
                },
                hover_data=['factor_pair']
            )
            fig.update_layout(height=500)
            st.plotly_chart(fig, use_container_width=True)
    
    # Key insights summary
    if 'key_insights' in factor_results:
        insights = factor_results['key_insights']
        
        st.markdown("#### ğŸ’¡ Key Discoveries")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric(
                "ğŸ† Top Safety Factor",
                insights.get('primary_factor', 'Speed'),
                help=f"Most important factor during {data_period_info.lower()}"
            )
        
        with col2:
            st.metric(
                "ğŸŒŸ Best Conditions",
                insights.get('optimal_conditions', 'Clear Weather'),
                help=f"Safest conditions during {data_period_info.lower()}"
            )
        
        with col3:
            improvement = insights.get('improvement_potential', 0)
            st.metric(
                "ğŸš€ Improvement Potential",
                f"{improvement:.0f}% safer",
                help=f"Potential safety improvement for {data_period_info.lower()}"
            )
    
    # AI-generated insight with period context
    generate_dynamic_safety_factors_insight(factor_results, data_period_info)


# Enhanced helper functions for dynamic analysis

def get_data_period_info(time_series_df):
    """Get friendly description of the data period"""
    try:
        if time_series_df is None or len(time_series_df) == 0:
            return "Available Data"
        
        if 'date' in time_series_df.columns:
            time_series_df['date'] = pd.to_datetime(time_series_df['date'])
            start_date = time_series_df['date'].min()
            end_date = time_series_df['date'].max()
            
            days_diff = (end_date - start_date).days + 1
            
            if days_diff == 1:
                return f"Data from {start_date.strftime('%B %d, %Y')}"
            elif days_diff <= 7:
                return f"{days_diff} Days of Data"
            elif days_diff <= 31:
                return f"{days_diff} Days of Data"
            elif days_diff <= 90:
                return f"~{days_diff//30} Months of Data"
            else:
                return f"~{days_diff//30} Months of Data"
        else:
            return f"{len(time_series_df)} Data Points"
    except:
        return "Available Data"


def create_dynamic_safety_predictions(df, meaningful_features, options):
    """Create safety predictions using dynamic data and meaningful variables"""
    try:
        # Prepare meaningful feature matrix
        X = df[meaningful_features].fillna(df[meaningful_features].median())
        
        # Create intelligent safety target based on available data
        safety_target = create_dynamic_intelligent_safety_target(df, meaningful_features)
        
        if safety_target is None or len(safety_target) < 5:
            return None
        
        # Adjust model complexity based on data size
        n_estimators = min(100, max(10, len(df) // 2))
        max_depth = max(3, min(8, len(meaningful_features) // 2))
        
        # Train model with dynamic parameters
        if len(X) > 10:  # Need reasonable split
            X_train, X_test, y_train, y_test = train_test_split(X, safety_target, test_size=0.3, random_state=42)
        else:
            # Use all data for training if dataset is small
            X_train, X_test, y_train, y_test = X, X, safety_target, safety_target
        
        model = RandomForestRegressor(
            n_estimators=n_estimators, 
            random_state=42, 
            max_depth=max_depth,
            min_samples_split=max(2, len(X_train) // 10)
        )
        model.fit(X_train, y_train)
        
        # Get predictions and feature importance
        predictions = model.predict(X_test)
        
        # Create user-friendly feature names
        feature_importance = pd.DataFrame({
            'feature': meaningful_features,
            'importance': model.feature_importances_,
            'friendly_name': [make_feature_friendly(f) for f in meaningful_features]
        }).sort_values('importance', ascending=True)
        
        accuracy = r2_score(y_test, predictions) if len(set(y_test)) > 1 else 0.5
        
        return {
            'model': model,
            'predictions': predictions,
            'feature_importance': feature_importance,
            'accuracy': max(0, accuracy),  # Ensure non-negative
            'meaningful_features': meaningful_features,
            'data_size': len(df)
        }
        
    except Exception as e:
        logger.error(f"Error in dynamic safety predictions: {e}")
        return None


def create_dynamic_intelligent_safety_target(df, meaningful_features):
    """Create intelligent safety target based on available meaningful variables"""
    try:
        # Look for incident-based targets first (most reliable)
        safety_indicators = ['incidents', 'braking_events', 'swerving_events', 'avg_braking_events', 'avg_swerving_events']
        
        for indicator in safety_indicators:
            if indicator in meaningful_features:
                incidents = df[indicator].fillna(df[indicator].median())
                if incidents.std() > 0:  # Has variation
                    return 1 / (1 + incidents)  # Inverse relationship - fewer incidents = higher safety
        
        # Look for intensity/severity based targets
        intensity_indicators = ['intensity', 'severity', 'severity_score']
        
        for indicator in intensity_indicators:
            if indicator in meaningful_features:
                intensity = df[indicator].fillna(df[indicator].median())
                if intensity.std() > 0:
                    return 1 / (1 + intensity)
        
        # Use speed-based safety (moderate speed is safest)
        if 'avg_speed' in meaningful_features:
            speed = df['avg_speed'].fillna(df['avg_speed'].median())
            if speed.std() > 0:
                # Optimal speed is around median, with penalty for extremes
                optimal_speed = speed.median()
                speed_safety = 1 - abs(speed - optimal_speed) / (speed.max() - speed.min() + 0.1)
                return np.clip(speed_safety, 0.1, 1.0)
        
        # Composite safety score from multiple factors
        safety_components = []
        
        # Weather safety (clear conditions are safer)
        if 'precipitation_mm' in meaningful_features:
            rain = df['precipitation_mm'].fillna(df['precipitation_mm'].median())
            rain_safety = 1 / (1 + rain)
            safety_components.append(rain_safety)
        
        if 'wind_speed' in meaningful_features:
            wind = df['wind_speed'].fillna(df['wind_speed'].median())
            wind_safety = 1 / (1 + wind / 10)  # Normalize wind
            safety_components.append(wind_safety)
        
        # Time-based safety (consistent riding is safer)
        if 'total_rides' in meaningful_features:
            rides = df['total_rides'].fillna(df['total_rides'].median())
            if rides.std() > 0:
                # Moderate number of rides is optimal
                optimal_rides = rides.median()
                ride_safety = 1 - abs(rides - optimal_rides) / (rides.max() - rides.min() + 0.1)
                safety_components.append(ride_safety)
        
        if len(safety_components) > 0:
            return np.mean(safety_components, axis=0)
        else:
            # Last resort: create synthetic target based on feature variation
            return np.random.uniform(0.3, 0.9, len(df))
                
    except Exception as e:
        logger.error(f"Error creating dynamic safety target: {e}")
        return None


def analyze_dynamic_cycling_dna(df, meaningful_features, n_clusters, data_period_info):
    """Analyze cycling patterns dynamically based on filtered data"""
    try:
        # Prepare feature matrix
        X = df[meaningful_features].fillna(df[meaningful_features].median())
        
        # Adjust number of clusters based on data size
        effective_clusters = min(n_clusters, max(2, len(df) // 3))
        
        # Standardize features
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # Perform clustering with validation
        if len(df) < effective_clusters:
            effective_clusters = max(2, len(df) // 2)
        
        kmeans = KMeans(n_clusters=effective_clusters, random_state=42, n_init=10)
        clusters = kmeans.fit_predict(X_scaled)
        
        # Validate clustering quality
        if len(set(clusters)) > 1:
            silhouette_avg = silhouette_score(X_scaled, clusters)
            if silhouette_avg < 0.2:  # Poor clustering
                effective_clusters = 2
                kmeans = KMeans(n_clusters=effective_clusters, random_state=42, n_init=10)
                clusters = kmeans.fit_predict(X_scaled)
        
        # Create cycling personas based on period data
        personas = create_dynamic_cycling_personas(df, meaningful_features, clusters, effective_clusters, data_period_info)
        
        # Analyze persona distribution
        unique_clusters, cluster_counts = np.unique(clusters, return_counts=True)
        persona_distribution = pd.DataFrame({
            'persona': [personas.get(i, f"Style {i}") for i in unique_clusters],
            'count': cluster_counts,
            'percentage': cluster_counts / len(clusters) * 100
        })
        
        # Generate personality traits based on period
        personality_traits = generate_dynamic_personality_traits(df, meaningful_features, clusters, data_period_info)
        
        # Create timeline if date data available and sufficient
        pattern_timeline = None
        if 'date' in df.columns and len(df) > 7:
            try:
                df_with_clusters = df.copy()
                df_with_clusters['cluster'] = clusters
                df_with_clusters['persona'] = [personas.get(c, f"Style {c}") for c in clusters]
                df_with_clusters['date'] = pd.to_datetime(df_with_clusters['date'])
                
                # Group by appropriate time period
                days_diff = (df_with_clusters['date'].max() - df_with_clusters['date'].min()).days
                if days_diff <= 14:
                    # Daily grouping for short periods
                    daily_patterns = df_with_clusters.groupby(
                        df_with_clusters['date'].dt.date
                    )['persona'].agg(lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else x.iloc[0]).reset_index()
                    daily_patterns['date'] = pd.to_datetime(daily_patterns['date'])
                    daily_patterns.columns = ['date', 'dominant_persona']
                    pattern_timeline = daily_patterns
                else:
                    # Weekly grouping for longer periods
                    weekly_patterns = df_with_clusters.groupby(
                        df_with_clusters['date'].dt.to_period('W')
                    )['persona'].agg(lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else x.iloc[0]).reset_index()
                    weekly_patterns['date'] = weekly_patterns['date'].dt.start_time
                    weekly_patterns.columns = ['date', 'dominant_persona']
                    pattern_timeline = weekly_patterns
            except Exception as e:
                logger.warning(f"Error creating pattern timeline: {e}")
                pattern_timeline = None
        
        return {
            'clusters': clusters,
            'personas': personas,
            'persona_distribution': persona_distribution,
            'personality_traits': personality_traits,
            'pattern_timeline': pattern_timeline,
            'n_patterns': effective_clusters,
            'clustering_quality': silhouette_avg if 'silhouette_avg' in locals() else 0.5
        }
        
    except Exception as e:
        logger.error(f"Error in dynamic cycling DNA analysis: {e}")
        return None


def create_dynamic_cycling_personas(df, meaningful_features, clusters, n_clusters, data_period_info):
    """Create meaningful cycling persona names based on dynamic cluster characteristics"""
    personas = {}
    
    try:
        for cluster_id in range(n_clusters):
            cluster_mask = clusters == cluster_id
            cluster_data = df[cluster_mask]
            
            if len(cluster_data) == 0:
                personas[cluster_id] = f"Unique Style {cluster_id}"
                continue
            
            # Analyze cluster characteristics dynamically
            persona_name = "ğŸš´â€â™€ï¸ Balanced Rider"  # Default
            
            # Speed-based classification
            if 'avg_speed' in meaningful_features:
                cluster_speed = cluster_data['avg_speed'].mean()
                overall_speed = df['avg_speed'].mean()
                
                if cluster_speed > overall_speed * 1.2:
                    persona_name = "âš¡ Speed Enthusiast"
                elif cluster_speed < overall_speed * 0.8:
                    persona_name = "ğŸŒ Leisurely Cruiser"
            
            # Safety-based classification
            safety_features = ['incidents', 'avg_braking_events', 'avg_swerving_events']
            for feature in safety_features:
                if feature in meaningful_features:
                    cluster_safety = cluster_data[feature].mean()
                    overall_safety = df[feature].mean()
                    
                    if cluster_safety > overall_safety * 1.3:
                        persona_name = "ğŸš¨ Careful Navigator" if 'braking' in feature else "ğŸš¨ Risk Aware"
                        break
                    elif cluster_safety < overall_safety * 0.7:
                        persona_name = "ğŸ›¡ï¸ Safety Champion"
                        break
            
            # Weather-based classification
            if 'precipitation_mm' in meaningful_features:
                cluster_rain = cluster_data['precipitation_mm'].mean()
                overall_rain = df['precipitation_mm'].mean()
                
                if cluster_rain > overall_rain * 1.5:
                    persona_name = "ğŸŒ§ï¸ Weather Warrior"
                elif cluster_rain < overall_rain * 0.3:
                    persona_name = "â˜€ï¸ Fair Weather Rider"
            
            # Activity level classification
            if 'total_rides' in meaningful_features:
                cluster_activity = cluster_data['total_rides'].mean()
                overall_activity = df['total_rides'].mean()
                
                if cluster_activity > overall_activity * 1.3:
                    persona_name = "ğŸš´â€â™‚ï¸ High Mileage Hero"
                elif cluster_activity < overall_activity * 0.7:
                    persona_name = "ğŸŒ± Casual Explorer"
            
            personas[cluster_id] = persona_name
        
        return personas
        
    except Exception as e:
        logger.error(f"Error creating dynamic personas: {e}")
        return {i: f"Style {i}" for i in range(n_clusters)}


def generate_dynamic_personality_traits(df, meaningful_features, clusters, data_period_info):
    """Generate personality traits based on dynamic cycling patterns"""
    traits = []
    
    try:
        # Speed analysis
        if 'avg_speed' in meaningful_features:
            avg_speed = df['avg_speed'].mean()
            speed_std = df['avg_speed'].std()
            
            if avg_speed > df['avg_speed'].median():
                traits.append(f"You preferred faster riding during {data_period_info.lower()}")
            else:
                traits.append(f"You maintained a comfortable pace during {data_period_info.lower()}")
            
            if speed_std < df['avg_speed'].mean() * 0.2:
                traits.append(f"Your speed was very consistent during {data_period_info.lower()}")
        
        # Safety analysis
        safety_features = ['incidents', 'avg_braking_events', 'avg_swerving_events']
        for feature in safety_features:
            if feature in meaningful_features:
                feature_mean = df[feature].mean()
                feature_median = df[feature].median()
                
                if feature_mean < feature_median:
                    traits.append(f"You had fewer safety events than typical during {data_period_info.lower()}")
                break
        
        # Weather analysis
        if 'precipitation_mm' in meaningful_features:
            total_rain_days = (df['precipitation_mm'] > 0).sum()
            total_days = len(df)
            rain_percentage = total_rain_days / total_days * 100
            
            if rain_percentage > 25:
                traits.append(f"You rode in varied weather conditions during {data_period_info.lower()}")
            elif rain_percentage < 10:
                traits.append(f"You chose mostly dry days during {data_period_info.lower()}")
        
        # Activity consistency
        if 'total_rides' in meaningful_features:
            rides_std = df['total_rides'].std()
            rides_mean = df['total_rides'].mean()
            
            if rides_std < rides_mean * 0.3:
                traits.append(f"You maintained consistent riding habits during {data_period_info.lower()}")
            else:
                traits.append(f"Your riding activity varied during {data_period_info.lower()}")
        
        # Ensure we have at least some traits
        if len(traits) == 0:
            traits = [
                f"You developed unique cycling patterns during {data_period_info.lower()}",
                "Your riding style shows interesting characteristics",
                "You're building consistent cycling habits"
            ]
        
        return traits[:4]  # Return top 4 traits
        
    except Exception as e:
        logger.error(f"Error generating dynamic traits: {e}")
        return [f"You had a unique cycling experience during {data_period_info.lower()}"]


def detect_dynamic_intelligent_alerts(df, meaningful_features, options, data_period_info):
    """Detect intelligent safety alerts using dynamic data"""
    try:
        # Prepare feature matrix
        X = df[meaningful_features].fillna(df[meaningful_features].median())
        
        # Adjust contamination based on data size
        contamination = min(options['anomaly_contamination'], max(0.01, 1.0 / len(df)))
        
        # Detect anomalies
        isolation_forest = IsolationForest(
            contamination=contamination,
            random_state=42,
            n_estimators=min(100, max(10, len(df)))
        )
        anomalies = isolation_forest.fit_predict(X)
        
        # Create intelligent alerts
        alert_mask = anomalies == -1
        alert_data = df[alert_mask].copy()
        
        total_alerts = len(alert_data)
        safe_days = len(df) - total_alerts
        
        if total_alerts == 0:
            return {
                'priority_alerts': [],
                'summary_stats': {
                    'period_alerts': 0, 
                    'safe_percentage': 100,
                    'data_period': data_period_info
                }
            }
        
        # Generate intelligent alert descriptions
        priority_alerts = []
        for _, row in alert_data.iterrows():
            alert = generate_dynamic_intelligent_alert_description(row, meaningful_features, df, data_period_info)
            priority_alerts.append(alert)
        
        # Sort by severity
        priority_alerts.sort(key=lambda x: x['severity'], reverse=True)
        
        # Create timeline
        alert_timeline = create_dynamic_alert_timeline(alert_data, df)
        
        # Calculate dynamic summary stats
        safe_percentage = (safe_days / len(df)) * 100
        summary_stats = {
            'period_alerts': total_alerts,
            'safe_percentage': safe_percentage,
            'data_period': data_period_info,
            'anomaly_rate': (total_alerts / len(df)) * 100
        }
        
        return {
            'priority_alerts': priority_alerts,
            'alert_timeline': alert_timeline,
            'summary_stats': summary_stats
        }
        
    except Exception as e:
        logger.error(f"Error in dynamic intelligent alerts: {e}")
        return None


def analyze_dynamic_intelligent_safety_factors(df, meaningful_features, data_period_info):
    """Analyze safety factors using dynamic data and meaningful variables"""
    try:
        # Calculate meaningful correlations
        feature_matrix = df[meaningful_features].fillna(df[meaningful_features].median())
        correlation_matrix = feature_matrix.corr()
        
        # Create safety target for factor analysis
        safety_target = create_dynamic_intelligent_safety_target(df, meaningful_features)
        
        if safety_target is None:
            return None
        
        # Calculate factor rankings based on correlation with safety
        factor_rankings = []
        for feature in meaningful_features:
            try:
                correlation_with_safety = np.corrcoef(feature_matrix[feature], safety_target)[0, 1]
                if not np.isnan(correlation_with_safety):
                    impact_score = abs(correlation_with_safety)
                    
                    factor_rankings.append({
                        'factor_name': make_feature_friendly(feature),
                        'impact_score': impact_score,
                        'correlation': correlation_with_safety
                    })
            except:
                continue
        
        if len(factor_rankings) == 0:
            return None
        
        factor_rankings_df = pd.DataFrame(factor_rankings)
        factor_rankings_df = factor_rankings_df.sort_values('impact_score', ascending=True)
        
        # Find smart correlations between meaningful factors
        smart_correlations = find_dynamic_smart_correlations(correlation_matrix, meaningful_features)
        
        # Generate key insights based on period data
        key_insights = generate_dynamic_factor_insights(factor_rankings_df, df, meaningful_features, data_period_info)
        
        return {
            'factor_rankings': factor_rankings_df,
            'smart_correlations': smart_correlations,
            'key_insights': key_insights,
            'correlation_matrix': correlation_matrix,
            'data_period': data_period_info
        }
        
    except Exception as e:
        logger.error(f"Error in dynamic safety factors analysis: {e}")
        return None


def make_feature_friendly(feature_name):
    """Convert technical feature names to user-friendly names"""
    friendly_names = {
        'avg_speed': 'ğŸƒâ€â™‚ï¸ Average Speed',
        'incidents': 'ğŸš¨ Safety Incidents',
        'avg_braking_events': 'ğŸš¦ Braking Frequency',
        'avg_swerving_events': 'â†©ï¸ Swerving Events',
        'temperature': 'ğŸŒ¡ï¸ Temperature',
        'precipitation_mm': 'ğŸŒ§ï¸ Rain Amount',
        'wind_speed': 'ğŸ’¨ Wind Speed',
        'visibility_km': 'ğŸ‘ï¸ Visibility',
        'total_rides': 'ğŸš´â€â™€ï¸ Daily Rides',
        'intensity': 'âš¡ Route Intensity',
        'incidents_count': 'ğŸ“Š Incident Count',
        'avg_deceleration': 'ğŸ›‘ Braking Force',
        'popularity_rating': 'â­ Route Popularity',
        'avg_duration': 'â±ï¸ Ride Duration',
        'distance_km': 'ğŸ“ Distance',
        'severity_score': 'ğŸ”¥ Severity Level'
    }
    
    return friendly_names.get(feature_name, feature_name.replace('_', ' ').title())


def find_dynamic_smart_correlations(correlation_matrix, meaningful_features):
    """Find meaningful correlations between factors in dynamic data"""
    correlations = []
    
    try:
        for i in range(len(meaningful_features)):
            for j in range(i+1, len(meaningful_features)):
                corr_val = correlation_matrix.iloc[i, j]
                
                if abs(corr_val) > 0.3 and not np.isnan(corr_val):  # Only meaningful correlations
                    factor1 = make_feature_friendly(meaningful_features[i])
                    factor2 = make_feature_friendly(meaningful_features[j])
                    
                    relationship_type = "Positive" if corr_val > 0 else "Negative"
                    
                    correlations.append({
                        'factor_pair': f"{factor1} â†” {factor2}",
                        'factor_1_impact': abs(corr_val),
                        'factor_2_impact': abs(corr_val),
                        'connection_strength': abs(corr_val),
                        'relationship_type': relationship_type
                    })
        
        return pd.DataFrame(correlations).sort_values('connection_strength', ascending=False) if correlations else pd.DataFrame()
        
    except Exception as e:
        logger.error(f"Error finding dynamic correlations: {e}")
        return pd.DataFrame()


def generate_dynamic_intelligent_alert_description(row, meaningful_features, full_df, data_period_info):
    """Generate intelligent, contextual alert descriptions for dynamic data"""
    try:
        date_str = row.get('date', datetime.now().strftime('%Y-%m-%d'))
        if hasattr(date_str, 'strftime'):
            date_str = date_str.strftime('%Y-%m-%d')
        
        # Calculate severity based on how unusual the values are
        severity = 0.5  # Base severity
        alert_reasons = []
        
        # Analyze what made this day unusual during the period
        for feature in meaningful_features:
            if feature in row:
                value = row[feature]
                if pd.notna(value):
                    feature_mean = full_df[feature].mean()
                    feature_std = full_df[feature].std()
                    
                    if feature_std > 0:
                        z_score = abs(value - feature_mean) / feature_std
                        if z_score > 2:  # More than 2 standard deviations
                            severity += 0.1
                            friendly_name = make_feature_friendly(feature)
                            alert_reasons.append(f"unusual {friendly_name.lower().replace('ğŸš¨', '').replace('ğŸš¦', '').replace('ğŸŒ§ï¸', '').strip()} ({value:.1f})")
        
        severity = min(0.9, severity)  # Cap at 0.9
        
        # Create contextual alert description
        if len(alert_reasons) > 0:
            main_reason = alert_reasons[0]
            title = f"Unusual Pattern Detected"
            description = f"During {data_period_info.lower()}, detected {main_reason}"
            if len(alert_reasons) > 1:
                description += f" plus {len(alert_reasons)-1} other anomalies"
        else:
            title = "Pattern Anomaly"
            description = f"Unusual combination of conditions detected during {data_period_info.lower()}"
        
        return {
            'date': str(date_str),
            'title': title,
            'description': description,
            'severity': severity,
            'factors': alert_reasons,
            'period': data_period_info
        }
        
    except Exception as e:
        logger.error(f"Error generating dynamic alert description: {e}")
        return {
            'date': datetime.now().strftime('%Y-%m-%d'),
            'title': 'Safety Alert',
            'description': f'Unusual pattern detected during {data_period_info.lower()}',
            'severity': 0.5,
            'factors': [],
            'period': data_period_info
        }


def create_dynamic_alert_timeline(alert_data, full_df):
    """Create timeline of alerts for the dynamic period"""
    try:
        if 'date' in alert_data.columns:
            alert_data['date'] = pd.to_datetime(alert_data['date'])
            
            # Get the full date range from the complete dataset
            full_df['date'] = pd.to_datetime(full_df['date'])
            date_range = pd.date_range(
                start=full_df['date'].min(),
                end=full_df['date'].max(),
                freq='D'
            )
            
            # Count alerts by date
            alert_counts = alert_data.groupby(alert_data['date'].dt.date).size().reset_index()
            alert_counts.columns = ['date', 'alert_count']
            
            # Create full timeline with zeros for days without alerts
            full_timeline = pd.DataFrame({'date': date_range.date})
            full_timeline = full_timeline.merge(alert_counts, on='date', how='left')
            full_timeline['alert_count'] = full_timeline['alert_count'].fillna(0)
            
            return full_timeline
        else:
            # Create simple timeline
            return pd.DataFrame({
                'date': [datetime.now().date()],
                'alert_count': [len(alert_data)]
            })
            
    except Exception as e:
        logger.error(f"Error creating dynamic timeline: {e}")
        return pd.DataFrame({'date': [datetime.now().date()], 'alert_count': [0]})


def generate_dynamic_factor_insights(factor_rankings_df, df, meaningful_features, data_period_info):
    """Generate key insights about safety factors for the dynamic period"""
    insights = {}
    
    try:
        # Primary safety factor
        if not factor_rankings_df.empty:
            top_factor = factor_rankings_df.iloc[-1]['factor_name']
            insights['primary_factor'] = top_factor.replace('ğŸš´â€â™‚ï¸', '').replace('ğŸƒâ€â™€ï¸', '').replace('âš¡', '').replace('ğŸŒ§ï¸', '').strip()
        
        # Optimal conditions based on period data
        optimal_conditions = "Clear Weather"
        if 'temperature' in meaningful_features:
            avg_temp = df['temperature'].mean()
            if avg_temp > 25:
                optimal_conditions = "Warm Conditions"
            elif avg_temp < 10:
                optimal_conditions = "Cool Conditions"
            else:
                optimal_conditions = "Moderate Weather"
        
        if 'precipitation_mm' in meaningful_features:
            avg_rain = df['precipitation_mm'].mean()
            if avg_rain < 1:
                optimal_conditions = "Dry Conditions"
            elif avg_rain > 5:
                optimal_conditions = "Varied Weather"
        
        insights['optimal_conditions'] = optimal_conditions
        
        # Improvement potential based on actual data variation
        if 'incidents' in meaningful_features:
            current_incidents = df['incidents'].mean()
            min_incidents = df['incidents'].quantile(0.1)
            if current_incidents > min_incidents:
                improvement = ((current_incidents - min_incidents) / current_incidents) * 100
                insights['improvement_potential'] = max(5, min(50, improvement))
            else:
                insights['improvement_potential'] = 10
        elif any(feature in meaningful_features for feature in ['avg_braking_events', 'avg_swerving_events']):
            # Use braking/swerving for improvement calculation
            for feature in ['avg_braking_events', 'avg_swerving_events']:
                if feature in meaningful_features:
                    current_events = df[feature].mean()
                    min_events = df[feature].quantile(0.1)
                    if current_events > min_events:
                        improvement = ((current_events - min_events) / current_events) * 100
                        insights['improvement_potential'] = max(5, min(40, improvement))
                        break
        else:
            insights['improvement_potential'] = 15
        
        insights['data_period'] = data_period_info
        
        return insights
        
    except Exception as e:
        logger.error(f"Error generating dynamic insights: {e}")
        return {
            'primary_factor': 'Speed',
            'optimal_conditions': 'Clear Weather',
            'improvement_potential': 20,
            'data_period': data_period_info
        }


# AI-Generated Dynamic Insight Functions

def generate_dynamic_safety_intelligence_insight(prediction_results, safety_scores, data_period_info):
    """Generate AI insight for safety intelligence with period context"""
    try:
        avg_score = np.mean(safety_scores)
        data_size = prediction_results.get('data_size', len(safety_scores))
        top_factors = prediction_results['feature_importance'].tail(3)['friendly_name'].tolist()
        
        if avg_score > 7:
            insight_tone = "excellent"
            improvement = "fine-tuning"
        elif avg_score > 5:
            insight_tone = "good"
            improvement = "optimizing"
        else:
            insight_tone = "developing"
            improvement = "improving"
        
        # Data quality assessment
        data_quality = "strong statistical confidence" if data_size > 30 else "moderate confidence" if data_size > 10 else "emerging patterns"
        
        insight_text = f"""
        ğŸ¯ **Your safety profile during {data_period_info.lower()} was {insight_tone}!** Based on {len(prediction_results['predictions'])} analyzed scenarios from this period, your average safety score was **{avg_score:.1f}/10**.
        
        ğŸ” **Period Analysis**: Your top 3 safety factors during {data_period_info.lower()} were **{', '.join(top_factors)}**. Focus on {improvement} these areas for maximum impact.
        
        ğŸ’¡ **Dynamic Insight**: Small improvements in your primary factor during similar conditions could boost your safety score by up to 15%! Analysis shows {data_quality} with {data_size} data points.
        """
        
        st.markdown(f"""
        <div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 20px; border-radius: 15px; color: white; margin: 20px 0;'>
        {insight_text}
        </div>
        """, unsafe_allow_html=True)
        
    except Exception as e:
        logger.error(f"Error generating dynamic safety insight: {e}")


def generate_dynamic_cycling_dna_insight(pattern_results, data_period_info):
    """Generate AI insight for cycling DNA with period context"""
    try:
        if 'persona_distribution' in pattern_results:
            top_persona = pattern_results['persona_distribution'].loc[
                pattern_results['persona_distribution']['percentage'].idxmax(), 'persona'
            ]
            percentage = pattern_results['persona_distribution']['percentage'].max()
        else:
            top_persona = "Balanced Rider"
            percentage = 60
        
        traits = pattern_results.get('personality_traits', [])
        trait_summary = traits[0] if traits else f"You developed unique patterns during {data_period_info.lower()}"
        
        clustering_quality = pattern_results.get('clustering_quality', 0.5)
        quality_assessment = "very clear patterns" if clustering_quality > 0.5 else "emerging patterns" if clustering_quality > 0.3 else "developing characteristics"
        
        insight_text = f"""
        ğŸ§¬ **During {data_period_info.lower()}, you were primarily a {top_persona}** - this represented **{percentage:.0f}%** of your riding style!
        
        ğŸ­ **Period Personality**: {trait_summary}. This pattern suggests you prioritized {"safety and consistency" if any(word in trait_summary.lower() for word in ["safe", "consistent"]) else "performance and exploration" if any(word in trait_summary.lower() for word in ["speed", "varied"]) else "balanced cycling"}.
        
        ğŸ“Š **Pattern Confidence**: Analysis shows {quality_assessment} during {data_period_info.lower()}. {"Your style was well-defined" if clustering_quality > 0.5 else "Your patterns are still developing"}.
        """
        
        st.markdown(f"""
        <div style='background: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%); padding: 20px; border-radius: 15px; color: #333; margin: 20px 0;'>
        {insight_text}
        </div>
        """, unsafe_allow_html=True)
        
    except Exception as e:
        logger.error(f"Error generating dynamic DNA insight: {e}")


def generate_dynamic_smart_alerts_insight(alert_results, data_period_info):
    """Generate AI insight for smart alerts with period context"""
    try:
        period_alerts = alert_results['summary_stats'].get('period_alerts', 0)
        safe_percentage = alert_results['summary_stats'].get('safe_percentage', 100)
        anomaly_rate = alert_results['summary_stats'].get('anomaly_rate', 0)
        
        if period_alerts == 0:
            alert_status = f"ğŸ‰ **Outstanding safety record during {data_period_info.lower()}!** No alerts detected."
            advice = "Your riding patterns were consistently safe throughout this period."
            trend_assessment = "excellent safety consistency"
        elif period_alerts <= 2:
            alert_status = f"âœ… **Great safety performance during {data_period_info.lower()}!** Only {period_alerts} alerts detected."
            advice = "You maintained good safety practices during this period."
            trend_assessment = "strong safety awareness"
        else:
            alert_status = f"âš ï¸ **{period_alerts} alerts detected during {data_period_info.lower()}** - this suggests some challenging conditions."
            advice = "Consider reviewing the specific dates and conditions that triggered alerts."
            trend_assessment = "variable conditions with optimization opportunities"
        
        insight_text = f"""
        {alert_status}
        
        ğŸ”¥ **Period Performance**: {safe_percentage:.0f}% of days during {data_period_info.lower()} were flagged as safe! 
        
        ğŸ§  **AI Assessment**: {advice} Analysis shows {trend_assessment} with a {anomaly_rate:.1f}% anomaly rate.
        
        ğŸ“Š **Period Context**: {"This period showed excellent safety patterns" if anomaly_rate <= 5 else "This period had some challenging conditions" if anomaly_rate <= 15 else "This period showed varied safety patterns"}.
        """
        
        st.markdown(f"""
        <div style='background: linear-gradient(135deg, #ffecd2 0%, #fcb69f 100%); padding: 20px; border-radius: 15px; color: #333; margin: 20px 0;'>
        {insight_text}
        </div>
        """, unsafe_allow_html=True)
        
    except Exception as e:
        logger.error(f"Error generating dynamic alerts insight: {e}")


def generate_dynamic_safety_factors_insight(factor_results, data_period_info):
    """Generate AI insight for safety factors with period context"""
    try:
        if not factor_results['factor_rankings'].empty:
            top_factor = factor_results['factor_rankings'].iloc[-1]['factor_name']
            top_impact = factor_results['factor_rankings'].iloc[-1]['impact_score']
        else:
            top_factor = "Speed Management"
            top_impact = 0.6
        
        key_insights = factor_results['key_insights']
        improvement_potential = key_insights.get('improvement_potential', 20)
        correlations_count = len(factor_results.get('smart_correlations', []))
        
        correlation_strength = "strong interconnected relationships" if correlations_count > 5 else "some meaningful connections" if correlations_count > 2 else "independent factor influences"
        
        insight_text = f"""
        âš—ï¸ **Period Discovery**: During {data_period_info.lower()}, **{top_factor}** had the strongest impact on your safety (influence score: {top_impact:.2f}).
        
        ğŸ¯ **Period Optimization**: Under optimal conditions during {data_period_info.lower()} ({key_insights.get('optimal_conditions', 'clear weather').lower()}), you could have been **{improvement_potential:.0f}% safer** than your period average.
        
        ğŸ”— **Period Patterns**: Analysis found {correlations_count} significant relationships between safety factors during {data_period_info.lower()}, indicating {correlation_strength}.
        
        ğŸš€ **Period Action Plan**: {"Focus on your top factor when conditions match this period" if improvement_potential > 15 else "Fine-tune your approach during similar periods"} for maximum safety improvement!
        """
        
        st.markdown(f"""
        <div style='background: linear-gradient(135deg, #e0c3fc 0%, #9bb5ff 100%); padding: 20px; border-radius: 15px; color: #333; margin: 20px 0;'>
        {insight_text}
        </div>
        """, unsafe_allow_html=True)
        
    except Exception as e:
        logger.error(f"Error generating dynamic factors insight: {e}")


# Keep the original function name for compatibility
def render_ml_insights_page():
    """Wrapper to maintain compatibility with existing code"""
    render_smart_insights_page()
